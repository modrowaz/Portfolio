{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df58a84b-9324-4db0-bf94-92a65923fd5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Predict electricity tariff based on demographic and electric usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67340ad6-f594-438c-b274-f0c5aeae52d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "energy = spark.read.csv('/FileStore/tables/energy_data-20.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cda6d1-e166-41d4-a593-75db4dfb76de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#energy = energy.sample(False, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1723936d-fd37-409e-affa-0185d7550c79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Age: integer (nullable = true)\n-- MaritalStatus: integer (nullable = true)\n-- AnnualConsumption: integer (nullable = true)\n-- DayNightConsumption: double (nullable = true)\n-- IncomeLevel: integer (nullable = true)\n-- DwellingArea: integer (nullable = true)\n-- HasChildren: boolean (nullable = true)\n-- SolarRoof: boolean (nullable = true)\n-- ShiftableLoad: integer (nullable = true)\n-- RiskAttitude: boolean (nullable = true)\n-- AttitudeSustainability: integer (nullable = true)\n-- Tariff: string (nullable = true)\n\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">root\n |-- Age: integer (nullable = true)\n |-- MaritalStatus: integer (nullable = true)\n |-- AnnualConsumption: integer (nullable = true)\n |-- DayNightConsumption: double (nullable = true)\n |-- IncomeLevel: integer (nullable = true)\n |-- DwellingArea: integer (nullable = true)\n |-- HasChildren: boolean (nullable = true)\n |-- SolarRoof: boolean (nullable = true)\n |-- ShiftableLoad: integer (nullable = true)\n |-- RiskAttitude: boolean (nullable = true)\n |-- AttitudeSustainability: integer (nullable = true)\n |-- Tariff: string (nullable = true)\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "energy.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed87e91-2cb6-4da7-a3d3-038e338e91f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=['Age', 'MaritalStatus', 'AnnualConsumption', 'DayNightConsumption', 'IncomeLevel', 'DwellingArea', 'HasChildren', 'SolarRoof', 'ShiftableLoad', 'RiskAttitude', 'AttitudeSustainability'],\n",
    "  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd7654f-5911-4924-ba3f-692cf97fd052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Tariff\", outputCol=\"tariff_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f27b2b-aeee-4007-96a4-187b7663e818",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd42ef7-a756-4911-9eea-34634cd67f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"tariff_index\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc1d8c7-082c-4cf1-a710-1ecaaa787d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[assembler, indexer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc5cdf3-d77b-455d-9010-1cdf0a9645e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Classifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4682fc20-de65-4f4c-b974-685ecb4abbe3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"tariff_index\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af615600-03b9-422d-83ac-df6357577ef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc_pipeline = Pipeline(stages=[assembler, indexer, rfc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0590bc-bbda-456c-a702-e9d4616e7b07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf88d9b-3d08-4bf9-ab49-9fa1abfeb177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train,test = energy.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a9c1fd-e4a5-463f-b8fc-4d7549c7e3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417e3929-1f18-4076-8356-f5fc967ed80e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr_evaluator =  MulticlassClassificationEvaluator().setLabelCol(\"tariff_index\").setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e40115-b554-476b-83db-aa4de5bb2225",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.3, 0.7, 1])\n",
    "             .addGrid(lr.elasticNetParam, [0.4, 0.8, 1])\n",
    "             .build())\n",
    "\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=lr_evaluator,\n",
    "                          numFolds=5,\n",
    "                          seed=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672ee8a2-1dc2-4860-a114-d10922d1d42f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_cvModel = lr_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602ed08e-bff1-4396-9827-4b67fb0f77ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.3\n0.4\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0.3\n0.4\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (lr_cvModel.bestModel.stages[-1].getRegParam())\n",
    "print (lr_cvModel.bestModel.stages[-1].getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4801d6d0-6750-4c7d-9699-a9e48bdc103b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">accuracy = 0.5878594249201278\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">accuracy = 0.5878594249201278\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "lr_predictions = lr_cvModel.transform(test)\n",
    "acc = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"accuracy\"})\n",
    "print(\"accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f404624-d61c-447e-b5cf-a661380918eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45037450-e66c-4a46-8e7c-61617a94d356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rfc_evaluator =  MulticlassClassificationEvaluator().setLabelCol(\"tariff_index\").setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ee5ff4-ee97-4bf4-9a24-9ddf05593bfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "rfc_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rfc.maxDepth, [2, 7, 12])\n",
    "             .addGrid(rfc.numTrees, [2, 5, 10])\n",
    "             .build())\n",
    "\n",
    "rfc_crossval = CrossValidator(estimator=rfc_pipeline,\n",
    "                          estimatorParamMaps=rfc_paramGrid,\n",
    "                          evaluator=rfc_evaluator,\n",
    "                          numFolds=5,\n",
    "                          seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9067ec9-bc1b-4d1b-b3d1-dd82406b42e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc_cvModel = rfc_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f136b40-d5b8-4a36-9acc-4e8bb3c8b29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">7\n10\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">7\n10\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (rfc_cvModel.bestModel.stages[-1].getMaxDepth())\n",
    "print (rfc_cvModel.bestModel.stages[-1].getNumTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d80cb76-36d0-45f6-8754-504aa4245561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">accuracy = 0.7603833865814696\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">accuracy = 0.7603833865814696\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rfc_predictions = rfc_cvModel.transform(test)\n",
    "rfc_acc = rfc_evaluator.evaluate(rfc_predictions, {rfc_evaluator.metricName: \"accuracy\"})\n",
    "print(\"accuracy = {}\".format(rfc_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77603874-294c-4f58-8e30-33ec60cce4da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression w/Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c7a540-0b24-46e7-bfd8-7ec6ca7e1c34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import mlflow\n",
    "\n",
    "def lr_objective_function(params):    \n",
    "  # set the hyperparameters that we want to tune\n",
    "  regParam = params[\"regParam\"]\n",
    "  elasticNetParam = params[\"elasticNetParam\"]\n",
    "\n",
    "  # create a grid with our hyperparameters\n",
    "  lr_grid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [regParam])\n",
    "    .addGrid(lr.elasticNetParam, [elasticNetParam])\n",
    "    .build())\n",
    "\n",
    "  # cross validate the set of hyperparameters\n",
    "  lr_cv = CrossValidator(estimator=lr_pipeline, estimatorParamMaps=lr_grid, evaluator=lr_evaluator, numFolds=3)\n",
    "  lr_cvModel = lr_cv.fit(train)\n",
    "\n",
    "  # get our average RMSE across all three folds\n",
    "  acc = -lr_cvModel.avgMetrics[0]\n",
    "\n",
    "  return {\"loss\": acc, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad3b299-372b-4618-b5fb-3f8f006075be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "  \"regParam\": hp.uniform(\"regParam\", 0.3, 1),\n",
    "  \"elasticNetParam\": hp.uniform(\"elasticNetParam\", 0.4, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5593a6d4-2171-4d9f-8894-380fc4dd5196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2023/03/09 19:37:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:37:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n\r  0%|          | 0/9 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r2023/03/09 19:37:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r  0%|          | 0/9 [00:06&lt;?, ?trial/s, best loss=?]\r 11%|█         | 1/9 [00:06&lt;00:55,  6.95s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:37:59 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 11%|█         | 1/9 [00:13&lt;00:55,  6.95s/trial, best loss: -0.5722133606217192]\r 22%|██▏       | 2/9 [00:13&lt;00:46,  6.63s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 22%|██▏       | 2/9 [00:19&lt;00:46,  6.63s/trial, best loss: -0.5722133606217192]\r 33%|███▎      | 3/9 [00:19&lt;00:37,  6.33s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 33%|███▎      | 3/9 [00:25&lt;00:37,  6.33s/trial, best loss: -0.5722133606217192]\r 44%|████▍     | 4/9 [00:25&lt;00:32,  6.42s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:18 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 44%|████▍     | 4/9 [00:32&lt;00:32,  6.42s/trial, best loss: -0.5722133606217192]\r 56%|█████▌    | 5/9 [00:32&lt;00:25,  6.42s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 56%|█████▌    | 5/9 [00:38&lt;00:25,  6.42s/trial, best loss: -0.5722133606217192]\r 67%|██████▋   | 6/9 [00:38&lt;00:19,  6.41s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:30 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 67%|██████▋   | 6/9 [00:45&lt;00:19,  6.41s/trial, best loss: -0.5722133606217192]\r 78%|███████▊  | 7/9 [00:45&lt;00:12,  6.40s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:37 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 78%|███████▊  | 7/9 [00:51&lt;00:12,  6.40s/trial, best loss: -0.5722133606217192]\r 89%|████████▉ | 8/9 [00:51&lt;00:06,  6.39s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:43 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 89%|████████▉ | 8/9 [00:58&lt;00:06,  6.39s/trial, best loss: -0.5722133606217192]\r100%|██████████| 9/9 [00:58&lt;00:00,  6.45s/trial, best loss: -0.5722133606217192]\r100%|██████████| 9/9 [00:58&lt;00:00,  6.45s/trial, best loss: -0.5722133606217192]\n2023/03/09 19:38:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2023/03/09 19:37:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:37:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n\r  0%|          | 0/9 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r2023/03/09 19:37:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r  0%|          | 0/9 [00:06&lt;?, ?trial/s, best loss=?]\r 11%|█         | 1/9 [00:06&lt;00:55,  6.95s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:37:59 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 11%|█         | 1/9 [00:13&lt;00:55,  6.95s/trial, best loss: -0.5722133606217192]\r 22%|██▏       | 2/9 [00:13&lt;00:46,  6.63s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 22%|██▏       | 2/9 [00:19&lt;00:46,  6.63s/trial, best loss: -0.5722133606217192]\r 33%|███▎      | 3/9 [00:19&lt;00:37,  6.33s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 33%|███▎      | 3/9 [00:25&lt;00:37,  6.33s/trial, best loss: -0.5722133606217192]\r 44%|████▍     | 4/9 [00:25&lt;00:32,  6.42s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:18 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 44%|████▍     | 4/9 [00:32&lt;00:32,  6.42s/trial, best loss: -0.5722133606217192]\r 56%|█████▌    | 5/9 [00:32&lt;00:25,  6.42s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 56%|█████▌    | 5/9 [00:38&lt;00:25,  6.42s/trial, best loss: -0.5722133606217192]\r 67%|██████▋   | 6/9 [00:38&lt;00:19,  6.41s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:30 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 67%|██████▋   | 6/9 [00:45&lt;00:19,  6.41s/trial, best loss: -0.5722133606217192]\r 78%|███████▊  | 7/9 [00:45&lt;00:12,  6.40s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:37 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 78%|███████▊  | 7/9 [00:51&lt;00:12,  6.40s/trial, best loss: -0.5722133606217192]\r 89%|████████▉ | 8/9 [00:51&lt;00:06,  6.39s/trial, best loss: -0.5722133606217192]\r                                                                                \r2023/03/09 19:38:43 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 89%|████████▉ | 8/9 [00:58&lt;00:06,  6.39s/trial, best loss: -0.5722133606217192]\r100%|██████████| 9/9 [00:58&lt;00:00,  6.45s/trial, best loss: -0.5722133606217192]\r100%|██████████| 9/9 [00:58&lt;00:00,  6.45s/trial, best loss: -0.5722133606217192]\n2023/03/09 19:38:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "import numpy as np\n",
    "\n",
    "mlflow.autolog(exclusive=False)\n",
    "\n",
    "# Creating a parent run\n",
    "with mlflow.start_run(run_name = \"lr_tpe\"):\n",
    "    num_evals = 9\n",
    "    trials = Trials()\n",
    "    best_hyperparam = fmin(fn=lr_objective_function, \n",
    "                             space=search_space,\n",
    "                             algo=tpe.suggest, \n",
    "                             max_evals=num_evals,\n",
    "                             trials=trials,\n",
    "                             rstate=np.random.default_rng(42)\n",
    "                            )\n",
    "    # get optimal hyperparameter values\n",
    "    best_regParam = best_hyperparam[\"regParam\"]\n",
    "    best_elasticNetParam = best_hyperparam[\"elasticNetParam\"]\n",
    "\n",
    "    # change RF to use optimal hyperparameter values (this is a stateful method)\n",
    "    lr.setRegParam(best_regParam)\n",
    "    lr.setElasticNetParam(best_elasticNetParam)\n",
    "\n",
    "      # train pipeline on entire training data - this will use the updated RF values\n",
    "    pipelineModel = lr_pipeline.fit(train)\n",
    "\n",
    "      # evaluate final model on test data\n",
    "    predDF = pipelineModel.transform(test)\n",
    "    acc = lr_evaluator.evaluate(predDF)\n",
    "\n",
    "      # Log param and metric for the final model\n",
    "    mlflow.log_param(\"regParam\", best_regParam)\n",
    "    mlflow.log_param(\"elasticNetParam\", best_elasticNetParam)\n",
    "    mlflow.log_metric(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e5de2e-63ee-4546-8421-7952e4277014",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.5878594249201278\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0.5878594249201278\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c3f731-8bd0-4328-921b-05e5a497527e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest w/Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53ae2d5-e4e1-4521-9c02-9d809db87b82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import mlflow\n",
    "\n",
    "def rfc_objective_function(params):    \n",
    "  # set the hyperparameters that we want to tune\n",
    "  maxDepth = params[\"maxDepth\"]\n",
    "  numTrees = params[\"numTrees\"]\n",
    "\n",
    "  # create a grid with our hyperparameters\n",
    "  rfc_grid = (ParamGridBuilder()\n",
    "    .addGrid(rfc.maxDepth, [maxDepth])\n",
    "    .addGrid(rfc.numTrees, [numTrees])\n",
    "    .build())\n",
    "\n",
    "  # cross validate the set of hyperparameters\n",
    "  rfc_cv = CrossValidator(estimator=rfc_pipeline, estimatorParamMaps=rfc_grid, evaluator=rfc_evaluator, numFolds=3)\n",
    "  rfc_cvModel = rfc_cv.fit(train)\n",
    "\n",
    "  # get our average RMSE across all three folds\n",
    "  acc = -rfc_cvModel.avgMetrics[0]\n",
    "\n",
    "  return {\"loss\": acc, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5ccec6-2a1c-4a9b-b026-47bfedc65727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "rfc_search_space = {\n",
    "  \"maxDepth\": hp.randint(\"maxDepth\", 2, 12),\n",
    "  \"numTrees\": hp.randint(\"numTrees\", 2, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74cfc4f-d06f-4498-90df-b9bb021e0224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2023/03/09 19:38:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:38:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n\r  0%|          | 0/9 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r2023/03/09 19:38:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r  0%|          | 0/9 [00:06&lt;?, ?trial/s, best loss=?]\r 11%|█         | 1/9 [00:06&lt;00:48,  6.10s/trial, best loss: -0.6752593020086888]\r                                                                                \r2023/03/09 19:38:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 11%|█         | 1/9 [00:10&lt;00:48,  6.10s/trial, best loss: -0.6752593020086888]\r 22%|██▏       | 2/9 [00:10&lt;00:36,  5.17s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:01 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 22%|██▏       | 2/9 [00:14&lt;00:36,  5.17s/trial, best loss: -0.7536670995046411]\r 33%|███▎      | 3/9 [00:14&lt;00:27,  4.65s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 33%|███▎      | 3/9 [00:19&lt;00:27,  4.65s/trial, best loss: -0.7536670995046411]\r 44%|████▍     | 4/9 [00:19&lt;00:23,  4.71s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 44%|████▍     | 4/9 [00:24&lt;00:23,  4.71s/trial, best loss: -0.7536670995046411]\r 56%|█████▌    | 5/9 [00:24&lt;00:19,  4.95s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 56%|█████▌    | 5/9 [00:30&lt;00:19,  4.95s/trial, best loss: -0.7536670995046411]\r 67%|██████▋   | 6/9 [00:30&lt;00:15,  5.26s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 67%|██████▋   | 6/9 [00:36&lt;00:15,  5.26s/trial, best loss: -0.7536670995046411]\r 78%|███████▊  | 7/9 [00:36&lt;00:10,  5.34s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 78%|███████▊  | 7/9 [00:39&lt;00:10,  5.34s/trial, best loss: -0.7536670995046411]\r 89%|████████▉ | 8/9 [00:39&lt;00:04,  4.84s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:32 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 89%|████████▉ | 8/9 [00:45&lt;00:04,  4.84s/trial, best loss: -0.7536670995046411]\r100%|██████████| 9/9 [00:45&lt;00:00,  5.21s/trial, best loss: -0.7536670995046411]\r100%|██████████| 9/9 [00:45&lt;00:00,  5.11s/trial, best loss: -0.7536670995046411]\n2023/03/09 19:39:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmpa09voqei/model&#39;\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2023/03/09 19:38:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:38:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n\r  0%|          | 0/9 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r2023/03/09 19:38:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r  0%|          | 0/9 [00:06&lt;?, ?trial/s, best loss=?]\r 11%|█         | 1/9 [00:06&lt;00:48,  6.10s/trial, best loss: -0.6752593020086888]\r                                                                                \r2023/03/09 19:38:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 11%|█         | 1/9 [00:10&lt;00:48,  6.10s/trial, best loss: -0.6752593020086888]\r 22%|██▏       | 2/9 [00:10&lt;00:36,  5.17s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:01 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 22%|██▏       | 2/9 [00:14&lt;00:36,  5.17s/trial, best loss: -0.7536670995046411]\r 33%|███▎      | 3/9 [00:14&lt;00:27,  4.65s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 33%|███▎      | 3/9 [00:19&lt;00:27,  4.65s/trial, best loss: -0.7536670995046411]\r 44%|████▍     | 4/9 [00:19&lt;00:23,  4.71s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 44%|████▍     | 4/9 [00:24&lt;00:23,  4.71s/trial, best loss: -0.7536670995046411]\r 56%|█████▌    | 5/9 [00:24&lt;00:19,  4.95s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 56%|█████▌    | 5/9 [00:30&lt;00:19,  4.95s/trial, best loss: -0.7536670995046411]\r 67%|██████▋   | 6/9 [00:30&lt;00:15,  5.26s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 67%|██████▋   | 6/9 [00:36&lt;00:15,  5.26s/trial, best loss: -0.7536670995046411]\r 78%|███████▊  | 7/9 [00:36&lt;00:10,  5.34s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 78%|███████▊  | 7/9 [00:39&lt;00:10,  5.34s/trial, best loss: -0.7536670995046411]\r 89%|████████▉ | 8/9 [00:39&lt;00:04,  4.84s/trial, best loss: -0.7536670995046411]\r                                                                                \r2023/03/09 19:39:32 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 2] No usable temporary directory found in [&#39;/tmp&#39;, &#39;/var/tmp&#39;, &#39;/usr/tmp&#39;, &#39;/databricks/driver&#39;]\n\n\r 89%|████████▉ | 8/9 [00:45&lt;00:04,  4.84s/trial, best loss: -0.7536670995046411]\r100%|██████████| 9/9 [00:45&lt;00:00,  5.21s/trial, best loss: -0.7536670995046411]\r100%|██████████| 9/9 [00:45&lt;00:00,  5.11s/trial, best loss: -0.7536670995046411]\n2023/03/09 19:39:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmpa09voqei/model&#39;\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "import numpy as np\n",
    "\n",
    "mlflow.autolog(exclusive=False)\n",
    "\n",
    "# Creating a parent run\n",
    "with mlflow.start_run(run_name = \"rfc_tpe\"):\n",
    "    num_evals = 9\n",
    "    trials = Trials()\n",
    "    best_hyperparam = fmin(fn=rfc_objective_function, \n",
    "                             space=rfc_search_space,\n",
    "                             algo=tpe.suggest, \n",
    "                             max_evals=num_evals,\n",
    "                             trials=trials,\n",
    "                             rstate=np.random.default_rng(42)\n",
    "                            )\n",
    "    # get optimal hyperparameter values\n",
    "    best_maxDepth = best_hyperparam[\"maxDepth\"]\n",
    "    best_numTrees = best_hyperparam[\"numTrees\"]\n",
    "\n",
    "    # change RF to use optimal hyperparameter values (this is a stateful method)\n",
    "    rfc.setMaxDepth(best_maxDepth)\n",
    "    rfc.setNumTrees(best_numTrees)\n",
    "\n",
    "      # train pipeline on entire training data - this will use the updated RF values\n",
    "    rfc_pipelineModel = rfc_pipeline.fit(train)\n",
    "\n",
    "      # evaluate final model on test data\n",
    "    predDF = rfc_pipelineModel.transform(test)\n",
    "    acc = rfc_evaluator.evaluate(predDF)\n",
    "\n",
    "      # Log param and metric for the final model\n",
    "    mlflow.log_param(\"max_depth\", best_maxDepth)\n",
    "    mlflow.log_param(\"num_trees\", best_numTrees)\n",
    "    mlflow.log_metric(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b07792-bf3d-4460-a91d-381a0c3cbfd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.7891373801916933\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0.7891373801916933\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7288c2-7072-46c9-84eb-6ca30cad98c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CV & Mlflow - Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbaff7f-9730-41c4-adcd-2ca61a185f16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2023/03/09 19:39:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:39:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/03/09 19:40:50 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmpaabh9c8b&#39;\naccuracy = 0.5878594249201278\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2023/03/09 19:39:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:39:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/03/09 19:40:50 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmpaabh9c8b&#39;\naccuracy = 0.5878594249201278\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.autolog(exclusive=False)\n",
    "\n",
    "with mlflow.start_run(run_name = \"lr_cv\"):\n",
    "\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "    lr_evaluator =  MulticlassClassificationEvaluator().setLabelCol(\"tariff_index\").setMetricName(\"accuracy\")\n",
    "\n",
    "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "    lr_paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [0.3, 0.7, 1])\n",
    "                 .addGrid(lr.elasticNetParam, [0.4, 0.8, 1])\n",
    "                 .build())\n",
    "\n",
    "    lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                              estimatorParamMaps=lr_paramGrid,\n",
    "                              evaluator=lr_evaluator,\n",
    "                              numFolds=5,\n",
    "                              seed=100) \n",
    "\n",
    "    lr_cvModel = lr_crossval.fit(train)\n",
    "\n",
    "    best_RegParam = lr_cvModel.bestModel.stages[-1].getRegParam()\n",
    "    best_NetParam = lr_cvModel.bestModel.stages[-1].getElasticNetParam()\n",
    "\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "    lr_predictions = lr_cvModel.transform(test)\n",
    "    acc = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"accuracy\"})\n",
    "    print(\"accuracy = {}\".format(acc))\n",
    "\n",
    "    mlflow.log_param(\"regparam\", best_RegParam)\n",
    "    mlflow.log_param(\"netparam\", best_NetParam)\n",
    "    mlflow.log_metric(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf70c6f-2c59-4596-9a1e-d7adfa6c53a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CV & Mlflow- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0820526f-625d-4b57-8aa3-9a4ec331b08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2023/03/09 19:40:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:40:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/03/09 19:41:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmptb1q3jbq&#39;\naccuracy = 0.7603833865814696\n</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2023/03/09 19:40:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/03/09 19:40:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/03/09 19:41:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: [Errno 28] No space left on device: &#39;/tmp/tmptb1q3jbq&#39;\naccuracy = 0.7603833865814696\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.autolog(exclusive=False)\n",
    "\n",
    "with mlflow.start_run(run_name = \"rfc_cv\"):\n",
    "    \n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "    rfc_evaluator =  MulticlassClassificationEvaluator().setLabelCol(\"tariff_index\").setMetricName(\"accuracy\")\n",
    "\n",
    "    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "    rfc_paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(rfc.maxDepth, [2, 7, 12])\n",
    "                 .addGrid(rfc.numTrees, [2, 5, 10])\n",
    "                 .build())\n",
    "\n",
    "    rfc_crossval = CrossValidator(estimator=rfc_pipeline,\n",
    "                              estimatorParamMaps=rfc_paramGrid,\n",
    "                              evaluator=rfc_evaluator,\n",
    "                              numFolds=5,\n",
    "                              seed=100)\n",
    "\n",
    "    rfc_cvModel = rfc_crossval.fit(train)\n",
    "\n",
    "    best_maxdepth = rfc_cvModel.bestModel.stages[-1].getMaxDepth()\n",
    "    best_numtrees = rfc_cvModel.bestModel.stages[-1].getNumTrees\n",
    "\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "    rfc_predictions = rfc_cvModel.transform(test)\n",
    "    rfc_acc = rfc_evaluator.evaluate(rfc_predictions, {rfc_evaluator.metricName: \"accuracy\"})\n",
    "    print(\"accuracy = {}\".format(rfc_acc))\n",
    "    \n",
    "    mlflow.log_param(\"maxdepth\", best_maxdepth)\n",
    "    mlflow.log_param(\"numtrees\", best_numtrees)\n",
    "    mlflow.log_metric(\"acc\", rfc_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45225b83-1fcd-4f1f-9154-05ac4bf2d862",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Out of the 4 models tracked, the random forest model with tpe implemented performed the best as it had the highest accuracy of 0.79. When comparing grid search and tpe methods for the random forest model, it was interesting that the grid search method suggested hyperparameters of 7 max depth and 10 number of trees, while the tpe method suggested 5 max depth and 8 number of trees. In terms of the logistic regression models, the tpe and grid search methods for hyperparameter tuning had almost identical accuracy score, however their configured parameters were different with the grid search log reg suggesting elastic net parameter of 0.4 while the tpe suggested a value of 0.77. In terms of regParam, the grid search suggested a value of 0.3 while the tpe suggested a value of 0.87. \n",
    "\n",
    "In terms of speed, the tpe log reg took 1.01 minutes to run while the grid search took 1.12 minutes to run. This is not surprising due to the Bayesian approach of tpe. However, the tpe of the random forest took 57 seconds to run while the grid search took 55.79 seconds to run. This is likely due to the fact that tree splitting is always dependent on the trees that come before it making a Bayesian approach tohyperparameter tuning nearly inconsequential in terms of run time. \n",
    "\n",
    "The model that should be implemented to predict tariff should be the random forest tpe. It has the highest accuracy out of all models and the difference in running time for the tpe and grid search models is very small. The hyperparameters for this model are a max depth of 5 and number of trees of 8."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "energy",
   "notebookOrigID": 3837761130889804,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
